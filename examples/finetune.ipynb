{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 25\n",
      "Dataset is shuffled...\n",
      "Dataset is splitted...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "train_path = \"make_data.json\" # from https://github.com/urchade/GLiNER/blob/main/examples/sample_data.json\n",
    "\n",
    "with open(train_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print('Dataset size:', len(data))\n",
    "#shuffle\n",
    "random.shuffle(data)    \n",
    "print('Dataset is shuffled...')\n",
    "\n",
    "train_data = data[:int(len(data)*0.9)]\n",
    "test_data = data[int(len(data)*0.9):]\n",
    "\n",
    "print('Dataset is splitted...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ners = list(set([j[-1] for i in train_data for j in i[\"ner\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5a983ddd4c49738964ef2c10f0a92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dothis/anaconda3/envs/csu/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# 현재 작업 디렉토리의 경로를 sys.path에 추가\n",
    "current_dir = os.getcwd()\n",
    "gliner_dir = os.path.join(current_dir, \"../\")\n",
    "\n",
    "# gliner 폴더를 sys.path에 추가하여 최우선 참조\n",
    "sys.path.insert(0, gliner_dir)\n",
    "\n",
    "from gliner import GLiNERConfig, GLiNER\n",
    "from gliner.data_processing.collator import DataCollatorWithPadding\n",
    "from gliner import GLiNER\n",
    "from gliner.training import Trainer, TrainingArguments\n",
    "from gliner.utils import load_config_as_namespace\n",
    "from gliner.data_processing import WordsSplitter, GLiNERDataset\n",
    "from gliner.data_processing import GLiNERDataset\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model_name = \"taeminlee/gliner_ko\"\n",
    "cache_dir = \"/home/dothis/dothis-ai/models/huggingface\"\n",
    "model = GLiNER.from_pretrained(model_name, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting all entities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 165366.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entity classes:  10\n",
      "Collecting all entities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 68385.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entity classes:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = GLiNERDataset(train_data, model.config, data_processor=model.data_processor)\n",
    "test_dataset = GLiNERDataset(test_data, model.config, data_processor=model.data_processor)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling transformer encoder...\n",
      "Compiling RNN...\n",
      "Compiling span representation layer...\n",
      "Compiling prompt representation layer...\n"
     ]
    }
   ],
   "source": [
    "#Optional: compile model for faster training\n",
    "torch.set_float32_matmul_precision('high')\n",
    "model.to(device)\n",
    "model.compile_for_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dothis/anaconda3/envs/csu/lib/python3.8/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "[2024-07-03 10:24:42,893] [3/0] torch._inductor.fx_passes.split_cat: [WARNING] example value absent for node: cat_2\n"
     ]
    }
   ],
   "source": [
    "import torch._dynamo\n",
    "\n",
    "# learning_rate는 학습 속도를 결정하는 중요한 하이퍼파라미터로, 모델이 학습 과정에서 얼마나 빠르게 가중치를 업데이트할지 결정합니다. \n",
    "# 현재 설정된 학습률은 5e-6이므로, 이를 조금 더 높은 값으로 변경해보세요. 예를 들어, 1e-5, 5e-5 또는 1e-4 등의 값을 시도할 수 있습니다. \n",
    "# 그러나 너무 큰 값으로 설정하면 학습이 불안정해질 수 있으므로 점진적으로 값을 증가시키는 것이 좋습니다.\n",
    "# warmup_ratio: 현재는 0.1로 설정되어 있는데, 이 값을 조정하여 학습 초기 단계에서 학습률을 점진적으로 증가시키는 양을 조정할 수 있습니다. \n",
    "# 예를 들어, 0.2로 증가시키면 더 천천히 학습률이 최대치에 도달하게 됩니다.\n",
    "# lr_scheduler_type: linear 외에도 cosine, cosine_with_restarts, polynomial 등 다양한 학습률 스케줄러를 시도해 볼 수 있습니다. \n",
    "# 각각의 스케줄러는 학습률을 조절하는 방식이 다르므로, 모델의 성능에 영향을 줄 수 있습니다.\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models\",\n",
    "    learning_rate=1e-6,\n",
    "    weight_decay=0.01,\n",
    "    others_lr=1e-5,\n",
    "    others_weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\", #cosine\n",
    "    warmup_ratio=0.2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_steps = 1000,\n",
    "    save_total_limit=10,\n",
    "    dataloader_num_workers = 8,\n",
    "    use_cpu = False,\n",
    "    report_to=\"none\",\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=model.data_processor.transformer_tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 끝난 후 모델 저장\n",
    "output_dir = \"models\"  # 저장할 디렉토리 경로 설정\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json not found in /home/dothis/dothis-ai/code/preprocessing/GLiNER/examples/models\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"models\"  # 저장할 디렉토리 경로 설정\n",
    "md = GLiNER.from_pretrained(output_dir, load_tokenizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새벽5시 => address => 0.08877214044332504\n",
      "\n",
      "PC방식당 => insurance company => 0.7874050736427307\n",
      "\n",
      "라면 => CIVILIZATION => 0.9401974678039551\n",
      "\n",
      "먹방 => credit card number => 0.05449414998292923\n",
      "\n",
      "간짜장 => CIVILIZATION => 0.9794231057167053\n",
      "\n",
      "열라면 => CIVILIZATION => 0.974287211894989\n",
      "\n",
      "탕수육 => CIVILIZATION => 0.9740515351295471\n",
      "\n",
      "까르보나라 => CIVILIZATION => 0.9535607099533081\n",
      "\n",
      "피자 => CIVILIZATION => 0.938845157623291\n",
      "\n",
      "돈까스 => CIVILIZATION => 0.9067108631134033\n",
      "\n",
      "삼겹살 => CIVILIZATION => 0.9629043340682983\n",
      "\n",
      "짜파게티 => CIVILIZATION => 0.9276237487792969\n",
      "\n",
      "비빔면 => CIVILIZATION => 0.9615005850791931\n",
      "\n",
      "김치볶음밥 => CIVILIZATION => 0.9626264572143555\n",
      "\n",
      "로제파스타 => CIVILIZATION => 0.9597557783126831\n",
      "\n",
      "고로케 => CIVILIZATION => 0.969813346862793\n",
      "\n",
      "소떡소떡 => CIVILIZATION => 0.9767699837684631\n",
      "\n",
      "와플 => CIVILIZATION => 0.9515304565429688\n",
      "\n",
      "ramen => CIVILIZATION => 0.6979901194572449\n",
      "\n",
      "mukbang => CIVILIZATION => 0.5251654386520386\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"새벽5시 PC방식당 라면 먹방 간짜장 열라면 탕수육 까르보나라 피자 돈까스 삼겹살 짜파게티 비빔면 김치볶음밥 로제파스타 고로케 소떡소떡 와플 ramen mukbang\"\n",
    "       \n",
    "\n",
    "# Labels for entity prediction\n",
    "\n",
    "tta_labels = [\"tokens\", \"artifacts\", \"person\", \"animal\", \"CIVILIZATION\", \"organization\", \\\n",
    "        \"phone number\", \"address\", \"passport number\", \"email\", \"credit card number\", \\\n",
    "        \"social security number\", \"health insurance id number\", 'Business/organization', \\\n",
    "        \"mobile phone number\", \"bank account number\", \"medication\", \"cpf\", \"driver's license number\", \\\n",
    "        \"tax identification number\", \"medical condition\", \"identity card number\", \"national id number\", \\\n",
    "        \"ip address\", \"email address\", \"iban\", \"credit card expiration date\", \"username\", \\\n",
    "        \"health insurance number\", \"student id number\", \"insurance number\", \"activity/event\", \\\n",
    "        \"landline phone number\", \"blood type\", \"cvv\", \"Food\", \"Brand\", \"resource\", \"FACILITY\", \\\n",
    "        \"digital signature\", \"social media handle\", \"license plate number\", \"cnpj\", \"postal code\", \\\n",
    "        \"passport_number\", \"vehicle registration number\", \"credit card brand\", \"food\", \"Restaurant\", \\\n",
    "        \"fax number\", \"visa number\", \"insurance company\", \"identity document number\", \\\n",
    "        \"national health insurance number\", \"cvc\", \"birth certificate number\", \"train ticket number\", \\\n",
    "        \"passport expiration date\", \"social_security_number\", \"EVENT\", \"STUDY_FIELD\", \"LOCATION\", \\\n",
    "        \"MATERIAL\", \"PLANT\", \"TERM\", \"THEORY\", 'Analysis Requirement', 'media content']\n",
    "# labels = [\"Person\", \"Award\"] # for v2.1 use capital case for better performance\n",
    "\n",
    "# Perform entity prediction\n",
    "entities = md.predict_entities(text, tta_labels, threshold=0.0)\n",
    "# entities = model.predict_entities(text, tta_labels, threshold=0.0)\n",
    "\n",
    "# Display predicted entities and their labels\n",
    "for entity in entities:\n",
    "    print(entity[\"text\"], \"=>\", entity[\"label\"], \"=>\", entity[\"score\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70cf4879e07542f29943d85e1539a2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새벽5시 => address => 0.0657939463853836\n",
      "\n",
      "PC방식당 => insurance company => 0.8107811212539673\n",
      "\n",
      "라면 => CIVILIZATION => 0.9729574918746948\n",
      "\n",
      "먹방 => credit card number => 0.004040951374918222\n",
      "\n",
      "간짜장 => CIVILIZATION => 0.9905232787132263\n",
      "\n",
      "열라면 => CIVILIZATION => 0.9863773584365845\n",
      "\n",
      "탕수육 => CIVILIZATION => 0.9846587181091309\n",
      "\n",
      "까르보나라 => CIVILIZATION => 0.9652066230773926\n",
      "\n",
      "피자 => CIVILIZATION => 0.9571497440338135\n",
      "\n",
      "돈까스 => CIVILIZATION => 0.9248909950256348\n",
      "\n",
      "삼겹살 => CIVILIZATION => 0.9747151136398315\n",
      "\n",
      "짜파게티 => CIVILIZATION => 0.9472574591636658\n",
      "\n",
      "비빔면 => CIVILIZATION => 0.973558247089386\n",
      "\n",
      "김치볶음밥 => CIVILIZATION => 0.9727054834365845\n",
      "\n",
      "로제파스타 => CIVILIZATION => 0.9755313992500305\n",
      "\n",
      "고로케 => CIVILIZATION => 0.9824250340461731\n",
      "\n",
      "소떡소떡 => CIVILIZATION => 0.9859128594398499\n",
      "\n",
      "와플 => CIVILIZATION => 0.9689881801605225\n",
      "\n",
      "ramen => CIVILIZATION => 0.7187020778656006\n",
      "\n",
      "mukbang => CIVILIZATION => 0.5153679251670837\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GLiNER.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "entities = model.predict_entities(text, tta_labels, threshold=0.0)\n",
    "\n",
    "# Display predicted entities and their labels\n",
    "for entity in entities:\n",
    "    print(entity[\"text\"], \"=>\", entity[\"label\"], \"=>\", entity[\"score\"])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
